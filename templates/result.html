<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prediction Result</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
</head>
<body class="container mt-5 text-center" >
    <h2>Intrusion Detection Result</h2>
    
    <div class="alert alert-info">
        <h4>Predicted Attack Type: <strong>{{ attack_type }}</strong></h4>
        <p>Confidence: <strong>{{ confidence }}%</strong></p>
    </div>

    <div class="alert alert-warning">
        <h5>Precautionary Measures:</h5>
        <p>{{ precaution }}</p>
    </div>

    <h4>Explainable AI (SHAP Plot and LIME Xpl)</h4>
    <div class="global-explanations">
    <h3>Global Feature Importance</h3>
    <img src="{{ shap_img }}" alt="SHAP Explanation" class="img-fluid mt-3" style="max-width: 80%;">
    <p>{{ global_explanation }}</p>
    <p>This shows which principal components most influence model decisions overall</p>
    </div>
    <h3>Local Feature Importance</h3>
    
    <img src="{{ shap_img1 }}" alt="SHAP Explanation" class="img-fluid mt-3" style="max-width: 80%;" >
    <p>{{ waterfall_explanation }}</p>
    <img src="{{ shap_img2 }}" alt="SHAP Explanation" class="img-fluid mt-3" style="max-width: 60%;max-height:50%;" >
    <p>{{ bar_explanation }}</p>
    <p>{{ pca_interpretation }}</p>

    <div class="alert alert-secondary">
        <h5>Feature Importance (LIME(Local Interpretable Model-agnostic Explanation) Explanation):</h5>
        {{ lime_html|safe }}
    </div>

    <div class="alert alert-secondary mt-3">
        <h5>Textual Explanation:</h5>
        <pre>{{ lime_text }}</pre>
    </div>
</div>
    
    <div class="mt-4">
        <a href="/predict" class="btn btn-primary">Try Again</a>
        <a href="/logout" class="btn btn-danger">Logout</a>
    </div>
</body>
</html>